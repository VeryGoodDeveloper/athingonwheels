# Vehicle Tracking System

## Overview

Comprehensive tracking and inventory management system to prevent duplicates, sync with source data, and enable future analytics.

---

## Tracking Metadata

Each vehicle includes these tracking fields:

```typescript
{
  scrapedAt: string;      // ISO timestamp when scraped
  sourceUrl: string;       // Original listing URL
  importBatch: string;     // Batch identifier (e.g., "2026-02-12-full")
  lastVerified: string;    // Last time verified on source
  isActive: boolean;       // Still exists on source site
}
```

---

## Scripts

### 1. Scraper (with tracking)
**File:** `scripts/scrape-batch.js`

Automatically adds tracking metadata during scraping:
- `scrapedAt` - current timestamp
- `sourceUrl` - the URL being scraped
- `importBatch` - date-based identifier
- `lastVerified` - same as scrapedAt initially
- `isActive` - true (just scraped)

**Usage:**
```bash
# Start Chrome DevTools
google-chrome --remote-debugging-port=9222 --no-sandbox --disable-dev-shm-usage &

# Run scraper
cd /home/ssystem/.openclaw/workspace/athingonwheels
node scripts/scrape-batch.js
```

### 2. Add Tracking Metadata (post-processing)
**File:** `scripts/add-tracking-metadata.js`

Adds tracking fields to existing scraped data (for data scraped before tracking system).

**Usage:**
```bash
node scripts/add-tracking-metadata.js
```

**Actions:**
- Creates backup (.backup.json)
- Adds missing tracking fields
- Preserves existing data

### 3. Sync Inventory
**File:** `scripts/sync-inventory.js`

Compares inventory with source site to detect changes.

**Usage:**
```bash
node scripts/sync-inventory.js
```

**Detects:**
- Duplicates (VIN or URL)
- Vehicles removed from source
- New vehicles to scrape
- Generates detailed report (`SYNC_REPORT.md`)

### 4. Remove Duplicates
**File:** `scripts/remove-duplicates.js`

Automatically removes duplicate vehicles from inventory.

**Usage:**
```bash
node scripts/remove-duplicates.js
```

**Strategy:**
- Deduplicates by VIN (primary)
- Deduplicates by URL (for vehicles without VIN)
- Keeps vehicle with most complete data
- Creates backup before modifying

**Scoring:**
- +10 for non-zero price
- +1 per image
- +5 for description
- +3 for Carfax URL
- +2 for specs

---

## Workflow

### Initial Scrape
```bash
# 1. Scrape all vehicles (tracking metadata included)
node scripts/scrape-batch.js

# 2. Check for issues
node scripts/sync-inventory.js

# 3. Remove any duplicates
node scripts/remove-duplicates.js
```

### Periodic Sync (Weekly/Monthly)
```bash
# 1. Update source URLs
# (Re-fetch sitemap.xml if needed)

# 2. Run sync to identify changes
node scripts/sync-inventory.js

# 3. Review SYNC_REPORT.md

# 4. Scrape new vehicles if any
# (Update scraper to only scrape new URLs)

# 5. Mark inactive vehicles
# (Manual or automated based on sync report)
```

### Add Tracking to Old Data
```bash
# If you have data without tracking metadata
node scripts/add-tracking-metadata.js
```

---

## Reports

### Sync Report (`SYNC_REPORT.md`)
Generated by `sync-inventory.js`:
- Summary statistics
- Duplicate detection results
- List of inactive vehicles
- New vehicles to scrape
- Recommendations

### Deduplication Summary
Console output from `remove-duplicates.js`:
- Original count
- VIN duplicates removed
- URL duplicates removed
- Final count

---

## Database Schema (Future)

Current: JSON file (`scrapedInventory.json`)

Future migration to PostgreSQL/MongoDB:

```sql
CREATE TABLE vehicles (
  id SERIAL PRIMARY KEY,
  vin VARCHAR(17) UNIQUE,
  source_url VARCHAR(500) UNIQUE,
  import_batch VARCHAR(50),
  scraped_at TIMESTAMP,
  last_verified TIMESTAMP,
  is_active BOOLEAN DEFAULT true,
  -- ... other vehicle fields
  created_at TIMESTAMP DEFAULT NOW(),
  updated_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_import_batch ON vehicles(import_batch);
CREATE INDEX idx_is_active ON vehicles(is_active);
CREATE INDEX idx_scraped_at ON vehicles(scraped_at);
```

---

## Analytics Queries (Future)

With tracking data, you can:

```javascript
// Vehicles by import batch
const batchVehicles = vehicles.filter(v => 
  v.importBatch === '2026-02-12-full'
);

// Vehicles scraped in date range
const recentScrapes = vehicles.filter(v => {
  const scraped = new Date(v.scrapedAt);
  return scraped >= startDate && scraped <= endDate;
});

// Inactive vehicles (removed from source)
const removed = vehicles.filter(v => !v.isActive);

// Stale verifications (not checked in 7 days)
const stale = vehicles.filter(v => {
  const verified = new Date(v.lastVerified);
  const daysSince = (Date.now() - verified) / (1000 * 60 * 60 * 24);
  return daysSince > 7;
});
```

---

## Best Practices

1. **Always backup before modifying**
   - Scripts auto-create backups
   - Keep at least 2 versions of data

2. **Run sync regularly**
   - Weekly for active inventory
   - Monthly minimum

3. **Verify new scrapes**
   - Check duplicate count
   - Validate sample of vehicles
   - Ensure prices extracted correctly

4. **Track batch identifiers**
   - Use date-based: `YYYY-MM-DD-description`
   - Examples: `2026-02-12-full`, `2026-02-20-new-only`

5. **Monitor inactive vehicles**
   - Review monthly
   - Consider hiding from shop (keep in database)
   - Use for market analysis

---

## File Structure

```
athingonwheels/
├── lib/
│   └── scrapedInventory.json          # Main inventory (with tracking)
├── scripts/
│   ├── scrape-batch.js                # Scraper (includes tracking)
│   ├── add-tracking-metadata.js       # Add tracking to old data
│   ├── sync-inventory.js              # Sync with source
│   ├── remove-duplicates.js           # Deduplicate
│   └── all-vehicle-urls.txt           # Source URLs (812)
├── TRACKING_SYSTEM.md                 # This file
├── SYNC_REPORT.md                     # Generated by sync
└── types/
    └── vehicle.ts                     # TypeScript types (with tracking)
```

---

## Next Steps

- [ ] Run initial scrape with tracking (in progress via sub-agent)
- [ ] Add tracking metadata to existing 100 vehicles (if needed)
- [ ] Test sync-inventory script
- [ ] Test remove-duplicates script
- [ ] Set up weekly cron for syncing
- [ ] Build admin dashboard for tracking metrics
- [ ] Consider database migration for better queries

---

*Last updated: 2026-02-12*
